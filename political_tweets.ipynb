{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import datetime\n",
    "import csv\n",
    "import urllib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "import pdfplumber\n",
    "import urllib\n",
    "import io\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "#from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from sklearn.manifold import TSNE\n",
    "from six.moves import cPickle\n",
    "import gensim.models.word2vec as w2v\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "# from tensorflow.python.framework import ops\n",
    "# ops.reset_default_graph()\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just usernames\n",
    "# https://www.kaggle.com/mrmorj/us-politicians-twitter-dataset\n",
    "df = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Twitter_username</th>\n",
       "      <th>Account_start_time</th>\n",
       "      <th>Account_ID</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Birthplace</th>\n",
       "      <th>Birthday</th>\n",
       "      <th>Age</th>\n",
       "      <th>Instagram_username</th>\n",
       "      <th>Political_party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A. Donald McEachin</td>\n",
       "      <td>RepMcEachin</td>\n",
       "      <td>2017-01-03T00:00:00Z</td>\n",
       "      <td>816181091673448448</td>\n",
       "      <td>male</td>\n",
       "      <td>Germany</td>\n",
       "      <td>1961-10-10T00:00:00Z</td>\n",
       "      <td>59.0</td>\n",
       "      <td>repmceachin</td>\n",
       "      <td>Democratic Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaron Michlewitz</td>\n",
       "      <td>RepMichlewitz</td>\n",
       "      <td>2010-06-27T00:00:00Z</td>\n",
       "      <td>160246973</td>\n",
       "      <td>male</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>1978-01-01T00:00:00Z</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Democratic Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aaron Peskin</td>\n",
       "      <td>AaronPeskin</td>\n",
       "      <td>2010-11-13T00:00:00Z</td>\n",
       "      <td>215369273</td>\n",
       "      <td>male</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>1964-06-17T00:00:00Z</td>\n",
       "      <td>56.0</td>\n",
       "      <td>apeskin52</td>\n",
       "      <td>Democratic Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Peña</td>\n",
       "      <td>AaronPena</td>\n",
       "      <td>2007-10-31T00:00:00Z</td>\n",
       "      <td>9843332</td>\n",
       "      <td>male</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>1959-06-08T00:00:00Z</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Republican Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aaron Schock</td>\n",
       "      <td>aaronschock</td>\n",
       "      <td>2009-03-12T00:00:00Z</td>\n",
       "      <td>23951197</td>\n",
       "      <td>male</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>1981-05-28T00:00:00Z</td>\n",
       "      <td>39.0</td>\n",
       "      <td>aaronschock</td>\n",
       "      <td>Republican Party</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name Twitter_username    Account_start_time  \\\n",
       "0  A. Donald McEachin      RepMcEachin  2017-01-03T00:00:00Z   \n",
       "1    Aaron Michlewitz    RepMichlewitz  2010-06-27T00:00:00Z   \n",
       "2        Aaron Peskin      AaronPeskin  2010-11-13T00:00:00Z   \n",
       "3          Aaron Peña        AaronPena  2007-10-31T00:00:00Z   \n",
       "4        Aaron Schock      aaronschock  2009-03-12T00:00:00Z   \n",
       "\n",
       "           Account_ID   Sex                Birthplace              Birthday  \\\n",
       "0  816181091673448448  male                   Germany  1961-10-10T00:00:00Z   \n",
       "1           160246973  male  United States of America  1978-01-01T00:00:00Z   \n",
       "2           215369273  male  United States of America  1964-06-17T00:00:00Z   \n",
       "3             9843332  male  United States of America  1959-06-08T00:00:00Z   \n",
       "4            23951197  male  United States of America  1981-05-28T00:00:00Z   \n",
       "\n",
       "    Age Instagram_username   Political_party  \n",
       "0  59.0        repmceachin  Democratic Party  \n",
       "1  42.0                NaN  Democratic Party  \n",
       "2  56.0          apeskin52  Democratic Party  \n",
       "3  61.0                NaN  Republican Party  \n",
       "4  39.0        aaronschock  Republican Party  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2514, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual tweets\n",
    "# https://www.reddit.com/r/datasets/comments/6fniik/over_one_million_tweets_collected_from_us/\n",
    "# this data is 3 years old... \n",
    "# json_file = open('data/US_PoliticalTweets/tweets.json')\n",
    "# data = json.load(json_file)\n",
    "# tweets = json.load(json_file)\n",
    "\n",
    "tweets = []\n",
    "for line in open('data/US_PoliticalTweets/tweets.json', 'r'):\n",
    "    tweets.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame.from_dict(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['contributors', 'coordinates', 'created_at', 'display_text_range',\n",
       "       'entities', 'favorite_count', 'favorited', 'geo', 'id', 'id_str',\n",
       "       'in_reply_to_screen_name', 'in_reply_to_status_id',\n",
       "       'in_reply_to_status_id_str', 'in_reply_to_user_id',\n",
       "       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place',\n",
       "       'retweet_count', 'retweeted', 'screen_name', 'source', 'text',\n",
       "       'truncated', 'user_id', 'possibly_sensitive', 'extended_entities',\n",
       "       'quoted_status_id', 'quoted_status_id_str', 'withheld_copyright',\n",
       "       'withheld_in_countries', 'withheld_scope'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of politicians\n",
    "len(set(tweets_df.screen_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 2281.412844036697\n",
      "median: 2691.0\n"
     ]
    }
   ],
   "source": [
    "# average num of tweets\n",
    "t_gp = tweets_df.groupby(['screen_name'])['id'].size().reset_index()\n",
    "print('avg: {}'.format(t_gp.id.mean()))\n",
    "print('median: {}'.format(t_gp.id.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most of these politicians (417 / 545) are in the other dataset which includes full names, party, age... \n",
    "len(set(tweets_df.screen_name).intersection(set(df.Twitter_username)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/madeline.campbell/Documents/GitHub/analysis_projects/political_tweets/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_load_or_process(filename, processor_fn, function_arg):\n",
    "    load_fn = None\n",
    "    save_fn = None\n",
    "    if filename.endswith(\"json\"):\n",
    "        load_fn = load_json\n",
    "        save_fn = save_json\n",
    "    else:\n",
    "        load_fn = load_bin\n",
    "        save_fn = save_bin\n",
    "    if os.path.exists(filename):\n",
    "        return load_fn(filename)\n",
    "    else:\n",
    "        ret = processor_fn(function_arg)\n",
    "        save_fn(ret, filename)\n",
    "        return ret\n",
    "\n",
    "def print_progress(current, maximum):\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    sys.stdout.flush()\n",
    "    sys.stdout.write(str(current) + \"/\" + str(maximum))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def save_bin(item, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        cPickle.dump(item, f)\n",
    "\n",
    "def load_bin(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return cPickle.load(f)\n",
    "\n",
    "def save_json(variable, filename):\n",
    "    with io.open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(variable, indent=4, ensure_ascii=False))\n",
    "\n",
    "def load_json(filename):\n",
    "    ret = None\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            with io.open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "                ret = json.load(f)\n",
    "        except:\n",
    "            pass\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          On a conference call about a weekend trip to I...\n",
       "1                                  Being interviewed by KTHV\n",
       "2          Being interviewed by KARN on his Arkansas Worl...\n",
       "3                      On KWHN in Fort Smith, that's 1320 AM\n",
       "4          Attending a Military of the Purple Heart Cerem...\n",
       "                                 ...                        \n",
       "1243365    Dismantling #DoddFrank returns us to the days ...\n",
       "1243366    In the shadows of the #ComeyHearing, @HouseGOP...\n",
       "1243367    @BetsyDeVosED How does a budget that cuts inve...\n",
       "1243368    Thank you @POTUS @NikkiHaley for strong stance...\n",
       "1243369    #WrongCHOICEAct will eliminate consumer protec...\n",
       "Name: text, Length: 1243370, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# might need to stip text of names (@..), urls, hashtags\n",
    "# try without stripping first? \n",
    "tweets_df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_df.text[0].split(' ')\n",
    "tweets_df['text_split'] = tweets_df.text.str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON file \n",
    "# 'data/stopwords.json'\n",
    "\n",
    "en_stopwords = [\"RT\", \"&amp;\", \"The\", \"w/\", \"We\", \"This\", \"...\", \"To\", \"&\", \"here:\", \"1\", \"As\", \"@\", \"2\", \"4\", \" \", \"-\", \"In\", \"My\", \"I'm\", \"A\", \"In\", \"It\", \"It's\", \"On\", \"If\", \"'ll\",\"'tis\",\"'twas\",\"'ve\",\"10\",\"39\",\"a\",\"a's\",\"able\",\"ableabout\",\"about\",\"above\",\"abroad\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"ad\",\"added\",\"adj\",\"adopted\",\"ae\",\"af\",\"affected\",\"affecting\",\"affects\",\"after\",\"afterwards\",\"ag\",\"again\",\"against\",\"ago\",\"ah\",\"ahead\",\"ai\",\"ain't\",\"aint\",\"al\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"alongside\",\"already\",\"also\",\"although\",\"always\",\"am\",\"amid\",\"amidst\",\"among\",\"amongst\",\"amoungst\",\"amount\",\"an\",\"and\",\"announce\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"ao\",\"apart\",\"apparently\",\"appear\",\"appreciate\",\"appropriate\",\"approximately\",\"aq\",\"ar\",\"are\",\"area\",\"areas\",\"aren\",\"aren't\",\"arent\",\"arise\",\"around\",\"arpa\",\"as\",\"aside\",\"ask\",\"asked\",\"asking\",\"asks\",\"associated\",\"at\",\"au\",\"auth\",\"available\",\"aw\",\"away\",\"awfully\",\"az\",\"b\",\"ba\",\"back\",\"backed\",\"backing\",\"backs\",\"backward\",\"backwards\",\"bb\",\"bd\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"began\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"being\",\"beings\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"bf\",\"bg\",\"bh\",\"bi\",\"big\",\"bill\",\"billion\",\"biol\",\"bj\",\"bm\",\"bn\",\"bo\",\"both\",\"bottom\",\"br\",\"brief\",\"briefly\",\"bs\",\"bt\",\"but\",\"buy\",\"bv\",\"bw\",\"by\",\"bz\",\"c\",\"c'mon\",\"c's\",\"ca\",\"call\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"caption\",\"case\",\"cases\",\"cause\",\"causes\",\"cc\",\"cd\",\"certain\",\"certainly\",\"cf\",\"cg\",\"ch\",\"changes\",\"ci\",\"ck\",\"cl\",\"clear\",\"clearly\",\"click\",\"cm\",\"cmon\",\"cn\",\"co\",\"co.\",\"com\",\"come\",\"comes\",\"computer\",\"con\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"copy\",\"corresponding\",\"could\",\"could've\",\"couldn\",\"couldn't\",\"couldnt\",\"course\",\"cr\",\"cry\",\"cs\",\"cu\",\"currently\",\"cv\",\"cx\",\"cy\",\"cz\",\"d\",\"dare\",\"daren't\",\"darent\",\"date\",\"de\",\"dear\",\"definitely\",\"describe\",\"described\",\"despite\",\"detail\",\"did\",\"didn\",\"didn't\",\"didnt\",\"differ\",\"different\",\"differently\",\"directly\",\"dj\",\"dk\",\"dm\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doesnt\",\"doing\",\"don\",\"don't\",\"done\",\"dont\",\"doubtful\",\"down\",\"downed\",\"downing\",\"downs\",\"downwards\",\"due\",\"during\",\"dz\",\"e\",\"each\",\"early\",\"ec\",\"ed\",\"edu\",\"ee\",\"effect\",\"eg\",\"eh\",\"eight\",\"eighty\",\"either\",\"eleven\",\"else\",\"elsewhere\",\"empty\",\"end\",\"ended\",\"ending\",\"ends\",\"enough\",\"entirely\",\"er\",\"es\",\"especially\",\"et\",\"et-al\",\"etc\",\"even\",\"evenly\",\"ever\",\"evermore\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"face\",\"faces\",\"fact\",\"facts\",\"fairly\",\"far\",\"farther\",\"felt\",\"few\",\"fewer\",\"ff\",\"fi\",\"fifteen\",\"fifth\",\"fifty\",\"fify\",\"fill\",\"find\",\"finds\",\"fire\",\"first\",\"five\",\"fix\",\"fj\",\"fk\",\"fm\",\"fo\",\"followed\",\"following\",\"follows\",\"for\",\"forever\",\"former\",\"formerly\",\"forth\",\"forty\",\"forward\",\"found\",\"four\",\"fr\",\"free\",\"from\",\"front\",\"full\",\"fully\",\"further\",\"furthered\",\"furthering\",\"furthermore\",\"furthers\",\"fx\",\"g\",\"ga\",\"gave\",\"gb\",\"gd\",\"ge\",\"general\",\"generally\",\"get\",\"gets\",\"getting\",\"gf\",\"gg\",\"gh\",\"gi\",\"give\",\"given\",\"gives\",\"giving\",\"gl\",\"gm\",\"gmt\",\"gn\",\"go\",\"goes\",\"going\",\"gone\",\"good\",\"goods\",\"got\",\"gotten\",\"gov\",\"gp\",\"gq\",\"gr\",\"great\",\"greater\",\"greatest\",\"greetings\",\"group\",\"grouped\",\"grouping\",\"groups\",\"gs\",\"gt\",\"gu\",\"gw\",\"gy\",\"h\",\"had\",\"hadn't\",\"hadnt\",\"half\",\"happens\",\"hardly\",\"has\",\"hasn\",\"hasn't\",\"hasnt\",\"have\",\"haven\",\"haven't\",\"havent\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"hed\",\"hell\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hers\",\"herself\",\"herse”\",\"hes\",\"hi\",\"hid\",\"high\",\"higher\",\"highest\",\"him\",\"himself\",\"himse”\",\"his\",\"hither\",\"hk\",\"hm\",\"hn\",\"home\",\"homepage\",\"hopefully\",\"how\",\"how'd\",\"how'll\",\"how's\",\"howbeit\",\"however\",\"hr\",\"ht\",\"htm\",\"html\",\"http\",\"hu\",\"hundred\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"i.e.\",\"id\",\"ie\",\"if\",\"ignored\",\"ii\",\"il\",\"ill\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"in\",\"inasmuch\",\"inc\",\"inc.\",\"indeed\",\"index\",\"indicate\",\"indicated\",\"indicates\",\"information\",\"inner\",\"inside\",\"insofar\",\"instead\",\"int\",\"interest\",\"interested\",\"interesting\",\"interests\",\"into\",\"invention\",\"inward\",\"io\",\"iq\",\"ir\",\"is\",\"isn\",\"isn't\",\"isnt\",\"it\",\"it'd\",\"it'll\",\"it's\",\"itd\",\"itll\",\"its\",\"itself\",\"itse”\",\"ive\",\"j\",\"je\",\"jm\",\"jo\",\"join\",\"jp\",\"just\",\"k\",\"ke\",\"keep\",\"keeps\",\"kept\",\"keys\",\"kg\",\"kh\",\"ki\",\"kind\",\"km\",\"kn\",\"knew\",\"know\",\"known\",\"knows\",\"kp\",\"kr\",\"kw\",\"ky\",\"kz\",\"l\",\"la\",\"large\",\"largely\",\"last\",\"lately\",\"later\",\"latest\",\"latter\",\"latterly\",\"lb\",\"lc\",\"least\",\"length\",\"less\",\"lest\",\"let\",\"let's\",\"lets\",\"li\",\"like\",\"liked\",\"likely\",\"likewise\",\"line\",\"little\",\"lk\",\"ll\",\"long\",\"longer\",\"longest\",\"look\",\"looking\",\"looks\",\"low\",\"lower\",\"lr\",\"ls\",\"lt\",\"ltd\",\"lu\",\"lv\",\"ly\",\"m\",\"ma\",\"made\",\"mainly\",\"make\",\"makes\",\"making\",\"man\",\"many\",\"may\",\"maybe\",\"mayn't\",\"maynt\",\"mc\",\"md\",\"me\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"member\",\"members\",\"men\",\"merely\",\"mg\",\"mh\",\"microsoft\",\"might\",\"might've\",\"mightn't\",\"mightnt\",\"mil\",\"mill\",\"million\",\"mine\",\"minus\",\"miss\",\"mk\",\"ml\",\"mm\",\"mn\",\"mo\",\"more\",\"moreover\",\"most\",\"mostly\",\"move\",\"mp\",\"mq\",\"mr\",\"mrs\",\"ms\",\"msie\",\"mt\",\"mu\",\"much\",\"mug\",\"must\",\"must've\",\"mustn't\",\"mustnt\",\"mv\",\"mw\",\"mx\",\"my\",\"myself\",\"myse”\",\"mz\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nc\",\"nd\",\"ne\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needed\",\"needing\",\"needn't\",\"neednt\",\"needs\",\"neither\",\"net\",\"netscape\",\"never\",\"neverf\",\"neverless\",\"nevertheless\",\"new\",\"newer\",\"newest\",\"next\",\"nf\",\"ng\",\"ni\",\"nine\",\"ninety\",\"nl\",\"no\",\"no-one\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"nor\",\"normally\",\"nos\",\"not\",\"noted\",\"nothing\",\"notwithstanding\",\"novel\",\"now\",\"nowhere\",\"np\",\"nr\",\"nu\",\"null\",\"number\",\"numbers\",\"nz\",\"o\",\"obtain\",\"obtained\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"older\",\"oldest\",\"om\",\"omitted\",\"on\",\"once\",\"one\",\"one's\",\"ones\",\"only\",\"onto\",\"open\",\"opened\",\"opening\",\"opens\",\"opposite\",\"or\",\"ord\",\"order\",\"ordered\",\"ordering\",\"orders\",\"org\",\"other\",\"others\",\"otherwise\",\"ought\",\"oughtn't\",\"oughtnt\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"owing\",\"own\",\"p\",\"pa\",\"page\",\"pages\",\"part\",\"parted\",\"particular\",\"particularly\",\"parting\",\"parts\",\"past\",\"pe\",\"per\",\"perhaps\",\"pf\",\"pg\",\"ph\",\"pk\",\"pl\",\"place\",\"placed\",\"places\",\"please\",\"plus\",\"pm\",\"pmid\",\"pn\",\"point\",\"pointed\",\"pointing\",\"points\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"pr\",\"predominantly\",\"present\",\"presented\",\"presenting\",\"presents\",\"presumably\",\"previously\",\"primarily\",\"probably\",\"problem\",\"problems\",\"promptly\",\"proud\",\"provided\",\"provides\",\"pt\",\"put\",\"puts\",\"pw\",\"py\",\"q\",\"qa\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"re\",\"readily\",\"really\",\"reasonably\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"reserved\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"ring\",\"ro\",\"room\",\"rooms\",\"round\",\"ru\",\"run\",\"rw\",\"s\",\"sa\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"sb\",\"sc\",\"sd\",\"se\",\"sec\",\"second\",\"secondly\",\"seconds\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"sees\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"seventy\",\"several\",\"sg\",\"sh\",\"shall\",\"shan't\",\"shant\",\"she\",\"she'd\",\"she'll\",\"she's\",\"shed\",\"shell\",\"shes\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"shouldnt\",\"show\",\"showed\",\"showing\",\"shown\",\"showns\",\"shows\",\"si\",\"side\",\"sides\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"sincere\",\"site\",\"six\",\"sixty\",\"sj\",\"sk\",\"sl\",\"slightly\",\"sm\",\"small\",\"smaller\",\"smallest\",\"sn\",\"so\",\"some\",\"somebody\",\"someday\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"sr\",\"st\",\"state\",\"states\",\"still\",\"stop\",\"strongly\",\"su\",\"sub\",\"substantially\",\"successfully\",\"such\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"sv\",\"sy\",\"system\",\"sz\",\"t\",\"t's\",\"take\",\"taken\",\"taking\",\"tc\",\"td\",\"tell\",\"ten\",\"tends\",\"test\",\"text\",\"tf\",\"tg\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that'll\",\"that's\",\"that've\",\"thatll\",\"thats\",\"thatve\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there'd\",\"there'll\",\"there're\",\"there's\",\"there've\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"therell\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"thereve\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"theyd\",\"theyll\",\"theyre\",\"theyve\",\"thick\",\"thin\",\"thing\",\"things\",\"think\",\"thinks\",\"third\",\"thirty\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"thou\",\"though\",\"thoughh\",\"thought\",\"thoughts\",\"thousand\",\"three\",\"throug\",\"through\",\"throughout\",\"thru\",\"thus\",\"til\",\"till\",\"tip\",\"tis\",\"tj\",\"tk\",\"tm\",\"tn\",\"to\",\"today\",\"together\",\"too\",\"took\",\"top\",\"toward\",\"towards\",\"tp\",\"tr\",\"tried\",\"tries\",\"trillion\",\"truly\",\"try\",\"trying\",\"ts\",\"tt\",\"turn\",\"turned\",\"turning\",\"turns\",\"tv\",\"tw\",\"twas\",\"twelve\",\"twenty\",\"twice\",\"two\",\"tz\",\"u\",\"ua\",\"ug\",\"uk\",\"um\",\"un\",\"under\",\"underneath\",\"undoing\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"ups\",\"upwards\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"uucp\",\"uy\",\"uz\",\"v\",\"va\",\"value\",\"various\",\"vc\",\"ve\",\"versus\",\"very\",\"vg\",\"vi\",\"via\",\"viz\",\"vn\",\"vol\",\"vols\",\"vs\",\"vu\",\"w\",\"want\",\"wanted\",\"wanting\",\"wants\",\"was\",\"wasn\",\"wasn't\",\"wasnt\",\"way\",\"ways\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"web\",\"webpage\",\"website\",\"wed\",\"welcome\",\"well\",\"wells\",\"went\",\"were\",\"weren\",\"weren't\",\"werent\",\"weve\",\"wf\",\"what\",\"what'd\",\"what'll\",\"what's\",\"what've\",\"whatever\",\"whatll\",\"whats\",\"whatve\",\"when\",\"when'd\",\"when'll\",\"when's\",\"whence\",\"whenever\",\"where\",\"where'd\",\"where'll\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"whichever\",\"while\",\"whilst\",\"whim\",\"whither\",\"who\",\"who'd\",\"who'll\",\"who's\",\"whod\",\"whoever\",\"whole\",\"wholl\",\"whom\",\"whomever\",\"whos\",\"whose\",\"why\",\"why'd\",\"why'll\",\"why's\",\"widely\",\"width\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won\",\"won't\",\"wonder\",\"wont\",\"words\",\"work\",\"worked\",\"working\",\"works\",\"world\",\"would\",\"would've\",\"wouldn\",\"wouldn't\",\"wouldnt\",\"ws\",\"www\",\"x\",\"y\",\"ye\",\"year\",\"years\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"youd\",\"youll\",\"young\",\"younger\",\"youngest\",\"your\",\"youre\",\"yours\",\"yourself\",\"yourselves\",\"youve\",\"yt\",\"yu\",\"z\",\"za\",\"zero\",\"zm\",\"zr\"]\n",
    "# remove stopwords\n",
    "# this list feels a little long but OK \n",
    "\n",
    "tweets_df['text_gowords'] = [' '.join([y for y in x if y not in en_stopwords]) for x in tweets_df['text_split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['textgo_split'] = tweets_df.text_gowords.str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conference', 'weekend', 'trip', 'Iraq', 'visit', 'Arkansas', 'troops']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.textgo_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequencies(corpus):\n",
    "    frequencies = Counter()\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            frequencies[word] += 1\n",
    "    freq = frequencies.most_common()\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting word frequencies\n",
      "Unique words: 1682639\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting word frequencies\")\n",
    "filename = os.path.join(save_dir, \"frequencies.json\")\n",
    "frequencies =  get_word_frequencies(tweets_df.textgo_split)\n",
    "vocab_size = len(frequencies)\n",
    "print(\"Unique words: \" + str(vocab_size))\n",
    "\n",
    "# word_freq = get_word_frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RT', 266209),\n",
       " ('&amp;', 216536),\n",
       " ('I', 127745),\n",
       " ('', 89389),\n",
       " ('The', 61207),\n",
       " ('We', 47356),\n",
       " ('-', 45323),\n",
       " ('w/', 43713),\n",
       " ('House', 40591),\n",
       " ('Great', 35350),\n",
       " ('Thanks', 33463),\n",
       " ('Thank', 32517),\n",
       " ('My', 29721),\n",
       " ('support', 29717),\n",
       " ('time', 27319),\n",
       " ('This', 26137),\n",
       " ('health', 23494),\n",
       " ('American', 23222),\n",
       " ('Congress', 23217),\n",
       " ('here:', 22308),\n",
       " ('people', 21197),\n",
       " ('Happy', 20849),\n",
       " ('Today', 20548),\n",
       " ('Senate', 19701),\n",
       " ('Americans', 19693),\n",
       " ('discuss', 19640),\n",
       " (\"I'm\", 19495),\n",
       " ('Act', 18398),\n",
       " ('2', 18162),\n",
       " ('President', 18156),\n",
       " ('Proud', 17554),\n",
       " ('today.', 17529),\n",
       " ('A', 17163),\n",
       " ('meeting', 16321),\n",
       " ('care', 16272),\n",
       " ('students', 16156),\n",
       " ('hearing', 15995),\n",
       " ('vote', 15979),\n",
       " ('U.S.', 15920),\n",
       " ('honor', 15613),\n",
       " ('In', 15499),\n",
       " ('US', 15424),\n",
       " ('It', 15293),\n",
       " ('day', 15252),\n",
       " ('Congrats', 15169),\n",
       " ('Watch', 15028),\n",
       " ('families', 14939),\n",
       " ('protect', 14834),\n",
       " ('jobs', 14660),\n",
       " ('office', 14481),\n",
       " (\"It's\", 14403),\n",
       " ('women', 14353),\n",
       " ('On', 14298),\n",
       " ('live', 13916),\n",
       " ('Read', 13856),\n",
       " ('talk', 13493),\n",
       " ('tax', 13430),\n",
       " ('morning', 13273),\n",
       " ('Our', 13051),\n",
       " ('fight', 12859),\n",
       " ('GOP', 12799),\n",
       " ('Obama', 12668),\n",
       " ('New', 12384),\n",
       " ('Rep.', 11963),\n",
       " ('passed', 11830),\n",
       " ('National', 11692),\n",
       " ('If', 11616),\n",
       " ('4', 11441),\n",
       " ('State', 11203),\n",
       " ('Tune', 11133),\n",
       " ('federal', 11129),\n",
       " ('continue', 11109),\n",
       " ('visit', 10961),\n",
       " ('bipartisan', 10815),\n",
       " ('plan', 10717),\n",
       " ('week', 10570),\n",
       " ('family', 10398),\n",
       " ('service', 10272),\n",
       " ('meet', 10266),\n",
       " ('–', 10253),\n",
       " ('Trump', 10252),\n",
       " ('Good', 10162),\n",
       " ('local', 10010),\n",
       " ('budget', 9948),\n",
       " ('...', 9797),\n",
       " ('law', 9790),\n",
       " ('public', 9763),\n",
       " ('stand', 9733),\n",
       " ('@', 9683),\n",
       " ('As', 9661),\n",
       " ('ICYMI:', 9546),\n",
       " ('community', 9456),\n",
       " ('County', 9441),\n",
       " ('Day', 9439),\n",
       " ('funding', 9318),\n",
       " ('statement', 9316),\n",
       " ('job', 9249),\n",
       " (\"I'll\", 9140),\n",
       " ('More', 9107),\n",
       " ('hear', 9040),\n",
       " ('legislation', 8881),\n",
       " ('Congratulations', 8880),\n",
       " ('floor', 8822),\n",
       " ('Congressional', 8816),\n",
       " ('You', 8764),\n",
       " ('America', 8737),\n",
       " ('Check', 8723),\n",
       " ('staff', 8677),\n",
       " ('access', 8594),\n",
       " ('What', 8568),\n",
       " ('voted', 8311),\n",
       " ('Honored', 8281),\n",
       " ('pass', 8267),\n",
       " ('@POTUS', 8234),\n",
       " ('#Obamacare', 8093),\n",
       " ('speaking', 7996),\n",
       " ('talking', 7996),\n",
       " ('#tcot', 7950),\n",
       " ('address', 7900),\n",
       " ('DC', 7867),\n",
       " ('Just', 7849),\n",
       " (\"today's\", 7819),\n",
       " ('tonight', 7769),\n",
       " ('Join', 7765),\n",
       " ('speak', 7702),\n",
       " ('country', 7683),\n",
       " ('ensure', 7675),\n",
       " ('economy', 7669),\n",
       " ('→', 7656),\n",
       " ('issues', 7652),\n",
       " ('future', 7641),\n",
       " ('Looking', 7601),\n",
       " ('Glad', 7599),\n",
       " ('national', 7556),\n",
       " ('leaders', 7420),\n",
       " ('Today,', 7402),\n",
       " ('news', 7361),\n",
       " ('Gov.', 7356),\n",
       " ('town', 7354),\n",
       " ('&', 7297),\n",
       " ('reform', 7268),\n",
       " ('For', 7227),\n",
       " ('create', 7137),\n",
       " ('veterans', 7129),\n",
       " ('today!', 7121),\n",
       " ('economic', 7111),\n",
       " ('Congressman', 7093),\n",
       " ('efforts', 7090),\n",
       " ('At', 7044),\n",
       " ('Washington', 7025),\n",
       " ('military', 7022),\n",
       " ('joined', 7001),\n",
       " ('celebrate', 6950),\n",
       " ('energy', 6858),\n",
       " ('colleagues', 6840),\n",
       " ('VA', 6833),\n",
       " ('1', 6653),\n",
       " ('security', 6635),\n",
       " ('pay', 6610),\n",
       " ('1st', 6593),\n",
       " ('To', 6574),\n",
       " ('gun', 6571),\n",
       " ('deserve', 6554),\n",
       " ('fighting', 6550),\n",
       " ('lives', 6530),\n",
       " ('Please', 6519),\n",
       " ('School', 6514),\n",
       " ('Sen.', 6465),\n",
       " ('Enjoyed', 6444),\n",
       " ('Committee', 6415),\n",
       " ('tomorrow', 6388),\n",
       " ('Capitol', 6364),\n",
       " (\"Let's\", 6222),\n",
       " ('government', 6217),\n",
       " ('business', 6196),\n",
       " ('supporting', 6116),\n",
       " ('action', 6106),\n",
       " ('+', 6092),\n",
       " ('--', 6091),\n",
       " ('safe', 6084),\n",
       " ('joining', 6077),\n",
       " ('I’m', 6048),\n",
       " ('friend', 6003),\n",
       " ('prayers', 6000),\n",
       " ('With', 5993),\n",
       " ('repeal', 5954),\n",
       " ('businesses', 5951),\n",
       " ('questions', 5940),\n",
       " ('hope', 5934),\n",
       " ('hard', 5934),\n",
       " ('leadership', 5913),\n",
       " ('lost', 5894),\n",
       " ('No', 5869),\n",
       " ('watch', 5815),\n",
       " ('read', 5810),\n",
       " ('Health', 5796),\n",
       " ('Republicans', 5777),\n",
       " ('Will', 5729),\n",
       " ('letter', 5699),\n",
       " ('nation', 5698),\n",
       " ('opportunity', 5673),\n",
       " ('serve', 5666),\n",
       " ('discussion', 5661),\n",
       " ('school', 5659),\n",
       " ('learn', 5644),\n",
       " ('tour', 5642),\n",
       " ('Bill', 5611),\n",
       " ('event', 5610),\n",
       " ('step', 5599),\n",
       " ('rights', 5586),\n",
       " ('coming', 5586),\n",
       " ('hosting', 5570),\n",
       " ('remember', 5570),\n",
       " ('It’s', 5557),\n",
       " ('Senator', 5548),\n",
       " ('Center', 5547),\n",
       " ('http…', 5477),\n",
       " ('story', 5468),\n",
       " ('signed', 5458),\n",
       " ('provide', 5445),\n",
       " ('it.', 5445),\n",
       " ('education', 5434),\n",
       " ('strong', 5403),\n",
       " ('improve', 5375),\n",
       " ('3', 5328),\n",
       " ('program', 5289),\n",
       " ('victims', 5287),\n",
       " ('ready', 5285),\n",
       " ('hall', 5283),\n",
       " ('receive', 5279),\n",
       " ('So', 5266),\n",
       " ('visiting', 5261),\n",
       " ('workers', 5250),\n",
       " ('helping', 5239),\n",
       " ('bring', 5238),\n",
       " ('spoke', 5229),\n",
       " ('@HouseGOP', 5223),\n",
       " ('honored', 5221),\n",
       " ('Learn', 5217),\n",
       " ('spending', 5199),\n",
       " ('discussing', 5174),\n",
       " ('cut', 5123),\n",
       " ('communities', 5122),\n",
       " ('student', 5120),\n",
       " ('Governor', 5104),\n",
       " ('birthday', 5099),\n",
       " ('interview', 5094),\n",
       " ('critical', 5085),\n",
       " ('report', 5079),\n",
       " ('children', 5067),\n",
       " ('Veterans', 5061),\n",
       " ('Had', 5010),\n",
       " ('life', 4989),\n",
       " ('change', 4979),\n",
       " ('#SOTU', 4976),\n",
       " ('days', 4952),\n",
       " ('met', 4946),\n",
       " ('water', 4942),\n",
       " ('happy', 4933),\n",
       " ('Act.', 4925),\n",
       " ('team', 4903),\n",
       " ('voting', 4883),\n",
       " ('millions', 4873),\n",
       " ('morning.', 4856),\n",
       " ('night', 4846),\n",
       " ('High', 4838),\n",
       " ('bills', 4823),\n",
       " ('MT', 4817),\n",
       " ('He', 4799),\n",
       " (\"Here's\", 4766),\n",
       " ('today,', 4754),\n",
       " ('htt…', 4735),\n",
       " ('hold', 4715),\n",
       " ('5', 4685),\n",
       " ('kids', 4675),\n",
       " ('#utpol', 4653),\n",
       " ('City', 4652),\n",
       " ('introduced', 4648),\n",
       " ('--&gt;', 4647),\n",
       " ('increase', 4646),\n",
       " ('special', 4636),\n",
       " (\"We're\", 4625),\n",
       " ('Thx', 4596),\n",
       " ('info', 4573),\n",
       " ('How', 4558),\n",
       " ('safety', 4557),\n",
       " ('healthcare', 4556),\n",
       " ('#ACA', 4541),\n",
       " ('LIVE', 4533),\n",
       " ('yesterday', 4531),\n",
       " ('Obamacare', 4510),\n",
       " ('anniversary', 4476),\n",
       " ('insurance', 4475),\n",
       " ('John', 4449),\n",
       " ('#SCOTUS', 4443),\n",
       " ('speech', 4440),\n",
       " ('share', 4439),\n",
       " ('Chairman', 4429),\n",
       " ('#mepolitics', 4428),\n",
       " ('impact', 4427),\n",
       " ('Dr.', 4424),\n",
       " ('lead', 4423),\n",
       " ('friends', 4412),\n",
       " ('policy', 4408),\n",
       " ('And', 4374),\n",
       " ('real', 4334),\n",
       " ('Pleased', 4325),\n",
       " ('cuts', 4318),\n",
       " ('Now', 4300),\n",
       " ('agree', 4287),\n",
       " ('…', 4268),\n",
       " ('North', 4258),\n",
       " ('including', 4257),\n",
       " ('op-ed', 4252),\n",
       " ('Last', 4236),\n",
       " ('start', 4228),\n",
       " ('White', 4196),\n",
       " ('decision', 4187),\n",
       " ('Hall', 4157),\n",
       " ('debt', 4152),\n",
       " ('coverage', 4144),\n",
       " ('Republican', 4126),\n",
       " ('violence', 4118),\n",
       " ('calling', 4093),\n",
       " ('you!', 4082),\n",
       " ('2016', 4078),\n",
       " ('win', 4068),\n",
       " ('They', 4055),\n",
       " ('power', 4052),\n",
       " ('video', 4050),\n",
       " ('families.', 4046),\n",
       " ('Hope', 4030),\n",
       " ('Joined', 4025),\n",
       " ('immigration', 4024),\n",
       " ('sign', 4019),\n",
       " ('Secretary', 4006),\n",
       " ('luck', 3969),\n",
       " ('Met', 3958),\n",
       " ('First', 3939),\n",
       " ('save', 3885),\n",
       " ('annual', 3882),\n",
       " ('United', 3877),\n",
       " ('holding', 3864),\n",
       " ('South', 3861),\n",
       " ('protecting', 3856),\n",
       " ('Time', 3850),\n",
       " ('Iran', 3850),\n",
       " ('https:/…', 3839),\n",
       " ('you.', 3837),\n",
       " ('prevent', 3826),\n",
       " ('https…', 3818),\n",
       " ('Care', 3814),\n",
       " ('District', 3788),\n",
       " ('hours', 3785),\n",
       " ('named', 3781),\n",
       " ('Here', 3757),\n",
       " ('bill.', 3750),\n",
       " ('folks', 3750),\n",
       " ('week.', 3741),\n",
       " ('political', 3739),\n",
       " ('Listen', 3730),\n",
       " ('When', 3722),\n",
       " ('There', 3719),\n",
       " ('abt', 3712),\n",
       " ('celebrating', 3698),\n",
       " ('https:…', 3695),\n",
       " (\"Obama's\", 3668),\n",
       " ('amendment', 3662),\n",
       " ('attack', 3658),\n",
       " ('Facebook', 3641),\n",
       " ('costs', 3619),\n",
       " ('more:', 3619),\n",
       " ('yrs', 3606),\n",
       " ('May', 3604),\n",
       " ('economy.', 3592),\n",
       " ('programs', 3576),\n",
       " ('press', 3566),\n",
       " ('From', 3566),\n",
       " ('college', 3564),\n",
       " ('See', 3562),\n",
       " ('ht…', 3561),\n",
       " ('cost', 3552),\n",
       " ('Dems', 3550),\n",
       " ('Big', 3548),\n",
       " ('country.', 3532),\n",
       " ('deal', 3528),\n",
       " ('benefits', 3516),\n",
       " ('Security', 3505),\n",
       " ('Court', 3503),\n",
       " ('commitment', 3497),\n",
       " ('Congress.', 3490),\n",
       " ('Wishing', 3473),\n",
       " ('continues', 3470),\n",
       " ('conference', 3469),\n",
       " (\"Trump's\", 3456),\n",
       " ('stay', 3455),\n",
       " ('Democrats', 3449),\n",
       " ('Texas', 3448),\n",
       " (\"I've\", 3447),\n",
       " ('https://…', 3447),\n",
       " ('@realDonaldTrump', 3447),\n",
       " ('|', 3432),\n",
       " ('Community', 3426),\n",
       " ('Excited', 3421),\n",
       " (\"Don't\", 3408),\n",
       " ('raise', 3407),\n",
       " ('now.', 3406),\n",
       " ('you,', 3394),\n",
       " ('Joining', 3383),\n",
       " ('Service', 3376),\n",
       " ('AM', 3367),\n",
       " ('passing', 3359),\n",
       " ('standing', 3357),\n",
       " ('All', 3349),\n",
       " ('One', 3343),\n",
       " ('https://t…', 3343),\n",
       " ('food', 3317),\n",
       " ('bad', 3294),\n",
       " ('combat', 3293),\n",
       " ('climate', 3283),\n",
       " ('Rep', 3281),\n",
       " ('West', 3267),\n",
       " ('Why', 3263),\n",
       " ('#jobs', 3262),\n",
       " ('Have', 3251),\n",
       " ('jobs.', 3249),\n",
       " ('Town', 3244),\n",
       " ('Gov', 3233),\n",
       " ('officials', 3230),\n",
       " ('check', 3220),\n",
       " ('constituents', 3219),\n",
       " ('love', 3218),\n",
       " ('clean', 3216),\n",
       " ('reduce', 3213),\n",
       " ('Be', 3208),\n",
       " ('affordable', 3208),\n",
       " ('crisis', 3205),\n",
       " ('Business', 3200),\n",
       " ('growth', 3200),\n",
       " ('grow', 3199),\n",
       " ('rule', 3199),\n",
       " ('class', 3197),\n",
       " ('resources', 3180),\n",
       " ('enjoyed', 3174),\n",
       " ('effort', 3166),\n",
       " ('role', 3159),\n",
       " ('After', 3158),\n",
       " ('residents', 3147),\n",
       " ('But', 3143),\n",
       " ('infrastructure', 3129),\n",
       " ('vets', 3128),\n",
       " ('build', 3124),\n",
       " ('in!', 3119),\n",
       " ('Your', 3115),\n",
       " ('2015', 3111),\n",
       " ('Not', 3110),\n",
       " ('fair', 3107),\n",
       " ('Another', 3099),\n",
       " ('#GOP', 3095),\n",
       " ('7', 3087),\n",
       " ('chance', 3085),\n",
       " ('6', 3082),\n",
       " ('trade', 3081),\n",
       " ('stopping', 3081),\n",
       " ('services', 3079),\n",
       " (\"America's\", 3074),\n",
       " ('tune', 3069),\n",
       " ('leading', 3068),\n",
       " ('Office', 3066),\n",
       " ('Award', 3065),\n",
       " ('expand', 3063),\n",
       " ('Supreme', 3058),\n",
       " ('calls', 3053),\n",
       " ('history', 3053),\n",
       " ('debate', 3042),\n",
       " ('helped', 3041),\n",
       " ('follow', 3029),\n",
       " ('Speaking', 3013),\n",
       " ('Very', 3009),\n",
       " ('Spoke', 3006),\n",
       " ('justice', 3004),\n",
       " ('money', 2997),\n",
       " ('close', 2991),\n",
       " ('employees', 2990),\n",
       " ('https://t.…', 2983),\n",
       " (\"President's\", 2979),\n",
       " ('2nd', 2972),\n",
       " ('left', 2969),\n",
       " ('#veterans', 2960),\n",
       " ('Americans.', 2958),\n",
       " ('VIDEO:', 2954),\n",
       " ('enforcement', 2952),\n",
       " ('plans', 2949),\n",
       " ('\"The', 2931),\n",
       " ('forget', 2929),\n",
       " ('update', 2924),\n",
       " ('brave', 2924),\n",
       " ('policies', 2895),\n",
       " ('serving', 2893),\n",
       " ('rural', 2892),\n",
       " ('General', 2889),\n",
       " ('.', 2889),\n",
       " ('https://t.c…', 2888),\n",
       " ('listen', 2884),\n",
       " ('opioid', 2880),\n",
       " ('Small', 2878),\n",
       " ('issue', 2877),\n",
       " ('seniors', 2876),\n",
       " ('progress', 2869),\n",
       " ('investigation', 2867),\n",
       " ('set', 2852),\n",
       " ('WATCH:', 2852),\n",
       " ('tonight.', 2851),\n",
       " ('Go', 2849),\n",
       " ('year.', 2849),\n",
       " ('voice', 2843),\n",
       " ('executive', 2832),\n",
       " ('industry', 2829),\n",
       " ('major', 2826),\n",
       " ('POTUS', 2826),\n",
       " ('strengthen', 2824),\n",
       " ('Director', 2821),\n",
       " ('EPA', 2817),\n",
       " (\"nation's\", 2816),\n",
       " ('record', 2814),\n",
       " ('human', 2813),\n",
       " ('announced', 2804),\n",
       " ('campaign', 2799),\n",
       " ('ET', 2794),\n",
       " ('#IRS', 2792),\n",
       " ('response', 2790),\n",
       " ('Act,', 2789),\n",
       " ('#4jobs', 2785),\n",
       " ('served', 2782),\n",
       " ('it’s', 2781),\n",
       " ('work.', 2766),\n",
       " ('100', 2766),\n",
       " ('online', 2761),\n",
       " ('Art', 2760),\n",
       " ('Memorial', 2759),\n",
       " ('grant', 2756),\n",
       " ('threat', 2753),\n",
       " ('Birthday', 2751),\n",
       " ('HR', 2750),\n",
       " ('glad', 2748),\n",
       " ('paid', 2739),\n",
       " ('St.', 2736),\n",
       " ('quality', 2735),\n",
       " ('week,', 2733),\n",
       " ('WATCH', 2726),\n",
       " ('afternoon', 2725),\n",
       " ('About', 2719),\n",
       " ('urge', 2718),\n",
       " ('fund', 2714),\n",
       " ('biz', 2714),\n",
       " ('Mr.', 2704),\n",
       " ('held', 2700),\n",
       " ('votes', 2699),\n",
       " ('equal', 2696),\n",
       " ('IRS', 2684),\n",
       " ('Floor', 2683),\n",
       " ('Women', 2678),\n",
       " (\"That's\", 2675),\n",
       " ('wonderful', 2672),\n",
       " ('day.', 2671),\n",
       " ('recognize', 2662),\n",
       " ('Judge', 2652),\n",
       " ('people.', 2649),\n",
       " ('Mayor', 2646),\n",
       " ('D.C.', 2646),\n",
       " ('drug', 2645),\n",
       " ('key', 2644),\n",
       " ('nuclear', 2643),\n",
       " ('host', 2643),\n",
       " ('growing', 2640),\n",
       " ('fellow', 2639),\n",
       " ('Do', 2639),\n",
       " ('law.', 2639),\n",
       " ('schools', 2638),\n",
       " ('honoring', 2638),\n",
       " ('States', 2636),\n",
       " ('leader', 2631),\n",
       " ('CT', 2631),\n",
       " ('News', 2622),\n",
       " ('Statement', 2617),\n",
       " ('helps', 2612),\n",
       " ('community.', 2593),\n",
       " ('https://t.co…', 2592),\n",
       " ('ceremony', 2590),\n",
       " ('Talking', 2589),\n",
       " ('opportunities', 2589),\n",
       " ('H.R.', 2577),\n",
       " ('\"We', 2576),\n",
       " ('Always', 2576),\n",
       " ('Fair', 2573),\n",
       " ('Are', 2549),\n",
       " ('companies', 2548),\n",
       " ('medical', 2544),\n",
       " ('Justice', 2543),\n",
       " ('historic', 2542),\n",
       " ('Federal', 2542),\n",
       " ('conversation', 2537),\n",
       " ('winning', 2536),\n",
       " ('Hill', 2533),\n",
       " ('Did', 2532),\n",
       " ('Park', 2528),\n",
       " ('taxes', 2526),\n",
       " ('San', 2524),\n",
       " ('Pres.', 2516),\n",
       " ('lose', 2516),\n",
       " ('true', 2516),\n",
       " ('them.', 2514),\n",
       " ('nation.', 2507),\n",
       " ('https://t.co/…', 2501),\n",
       " ('foreign', 2498),\n",
       " ('dangerous', 2497),\n",
       " ('district', 2496),\n",
       " ('Meeting', 2493),\n",
       " ('urging', 2492),\n",
       " ('creating', 2489),\n",
       " ('Ohio', 2485),\n",
       " ('concerns', 2481),\n",
       " ('Stop', 2472),\n",
       " ('Take', 2470),\n",
       " ('private', 2467),\n",
       " ('These', 2465),\n",
       " ('called', 2464),\n",
       " ('fed', 2463),\n",
       " ('freedom', 2459),\n",
       " ('loved', 2458),\n",
       " ('ban', 2452),\n",
       " ('takes', 2451),\n",
       " ('Day!', 2450),\n",
       " (\"week's\", 2447),\n",
       " ('Medicare', 2442),\n",
       " ('visited', 2440),\n",
       " ('send', 2439),\n",
       " ('beautiful', 2434),\n",
       " ('http:…', 2426),\n",
       " ('middle', 2419),\n",
       " ('An', 2419),\n",
       " ('rate', 2418),\n",
       " ('control', 2414),\n",
       " ('Keep', 2413),\n",
       " ('Welcome', 2411),\n",
       " ('en', 2408),\n",
       " ('Tax', 2407),\n",
       " ('care.', 2406),\n",
       " ('lot', 2404),\n",
       " ('troops', 2401),\n",
       " ('game', 2398),\n",
       " ('process', 2397),\n",
       " ('50', 2390),\n",
       " ('Administration', 2389),\n",
       " ('more.', 2389),\n",
       " ('Best', 2384),\n",
       " ('Energy', 2382),\n",
       " ('pleased', 2378),\n",
       " ('resolution', 2378),\n",
       " ('committed', 2377),\n",
       " ('photos', 2376),\n",
       " ('message', 2373),\n",
       " ('child', 2371),\n",
       " ('thousands', 2370),\n",
       " ('Middle', 2368),\n",
       " ('Joe', 2368),\n",
       " ('Click', 2368),\n",
       " ('PM', 2366),\n",
       " ('govt', 2364),\n",
       " ('Get', 2361),\n",
       " ('keeping', 2360),\n",
       " ('re:', 2356),\n",
       " ('moving', 2353),\n",
       " ('excited', 2351),\n",
       " ('wait', 2348),\n",
       " ('received', 2348),\n",
       " ('Caucus', 2345),\n",
       " ('president', 2343),\n",
       " (\"Today's\", 2339),\n",
       " ('21st', 2336),\n",
       " ('World', 2334),\n",
       " ('vital', 2334),\n",
       " ('pleasure', 2332),\n",
       " ('don’t', 2331),\n",
       " ('global', 2331),\n",
       " ('financial', 2331),\n",
       " ('amazing', 2331),\n",
       " ('@HouseDemocrats', 2331),\n",
       " ('rules', 2330),\n",
       " ('Follow', 2329),\n",
       " ('Budget', 2323),\n",
       " ('http://…', 2318),\n",
       " ('regulations', 2317),\n",
       " ('Find', 2315),\n",
       " ('attacks', 2315),\n",
       " ('@FoxNews', 2315),\n",
       " ('introduce', 2313),\n",
       " ('mental', 2312),\n",
       " ('http:/…', 2308),\n",
       " ('facing', 2304),\n",
       " ('Academy', 2300),\n",
       " ('Public', 2296),\n",
       " ('proposed', 2292),\n",
       " ('laws', 2291),\n",
       " ('weekend', 2288),\n",
       " ('moment', 2284),\n",
       " ('Every', 2282),\n",
       " ('funds', 2282),\n",
       " ('8', 2281),\n",
       " ('America.', 2280),\n",
       " ('Air', 2280),\n",
       " ('safe.', 2275),\n",
       " ('emergency', 2271),\n",
       " ('Sec.', 2271),\n",
       " ('award', 2267),\n",
       " ('Watch:', 2267),\n",
       " ('Day.', 2265),\n",
       " ('house', 2263),\n",
       " ('leave', 2260),\n",
       " ('failed', 2259),\n",
       " ('#immigration', 2258),\n",
       " ('sacrifice', 2256),\n",
       " ('focus', 2255),\n",
       " ('attend', 2252),\n",
       " ('Members', 2251),\n",
       " ('piece', 2248),\n",
       " ('abuse', 2245),\n",
       " ('discussed', 2240),\n",
       " ('Annual', 2238),\n",
       " ('Make', 2237),\n",
       " ('Medicaid', 2236),\n",
       " ('#FF', 2234),\n",
       " ('development', 2230),\n",
       " ('=', 2228),\n",
       " ('heard', 2226),\n",
       " ('Valley', 2225),\n",
       " ('defense', 2222),\n",
       " ('2014', 2222),\n",
       " ('committee', 2220),\n",
       " ('list', 2218),\n",
       " ('current', 2212),\n",
       " ('Council', 2207),\n",
       " ('Come', 2207),\n",
       " ('legislative', 2206),\n",
       " ('encourage', 2206),\n",
       " ('Visit', 2204),\n",
       " ('LIVE:', 2201),\n",
       " ('Military', 2197),\n",
       " ('Association', 2195),\n",
       " ('steps', 2192),\n",
       " ('Live', 2190),\n",
       " ('HS', 2188),\n",
       " ('NOW:', 2188),\n",
       " ('Let’s', 2188),\n",
       " ('http://t…', 2188),\n",
       " ('vote.', 2187),\n",
       " ('remain', 2187),\n",
       " ('a.m.', 2186),\n",
       " ('2017', 2182),\n",
       " ('jobs,', 2177),\n",
       " ('starting', 2172),\n",
       " ('training', 2171),\n",
       " ('final', 2169),\n",
       " ('weekly', 2168),\n",
       " ('photo', 2166),\n",
       " ('yesterday.', 2166),\n",
       " ('Central', 2166),\n",
       " ('March', 2164),\n",
       " ('solutions', 2162),\n",
       " ('Pres', 2158),\n",
       " ('live:', 2158),\n",
       " ('air', 2155),\n",
       " ('sharing', 2152),\n",
       " ('woman', 2151),\n",
       " ('Clinton', 2150),\n",
       " ('http://t.…', 2149),\n",
       " ('$', 2147),\n",
       " ('investment', 2144),\n",
       " ('service.', 2144),\n",
       " ('That', 2138),\n",
       " ('talks', 2137),\n",
       " ('politics', 2136),\n",
       " ('College', 2129),\n",
       " ('TY', 2126),\n",
       " ('h…', 2126),\n",
       " ('defend', 2125),\n",
       " ('grateful', 2125),\n",
       " ('@HouseGOP:', 2125),\n",
       " ('hurt', 2124),\n",
       " ('I’ll', 2123),\n",
       " ('force', 2122),\n",
       " ('April', 2121),\n",
       " ('actions', 2117),\n",
       " ('building', 2114),\n",
       " ('farmers', 2111),\n",
       " ('passage', 2109),\n",
       " ('Scott', 2108),\n",
       " ('missed', 2107),\n",
       " ('today’s', 2106),\n",
       " ('Russia', 2106),\n",
       " ('spend', 2105),\n",
       " ('wage', 2104),\n",
       " ('loan', 2102),\n",
       " ('war', 2099),\n",
       " ('@SpeakerRyan:', 2098),\n",
       " ('oppose', 2094),\n",
       " ('police', 2088),\n",
       " ('session', 2087),\n",
       " ('4th', 2086),\n",
       " ('proposal', 2083),\n",
       " ('joins', 2077),\n",
       " ('release', 2077),\n",
       " ('Social', 2074),\n",
       " ('nation’s', 2070),\n",
       " ('@WhiteHouse', 2065),\n",
       " ('@SpeakerBoehner:', 2065),\n",
       " ('panel', 2064),\n",
       " ('taxpayer', 2060),\n",
       " ('killed', 2059),\n",
       " ('@HouseCommerce:', 2051),\n",
       " ('trip', 2048),\n",
       " ('election', 2046),\n",
       " ('unemployment', 2042),\n",
       " ('Russian', 2042),\n",
       " ('Getting', 2041),\n",
       " ('challenges', 2040),\n",
       " ('#TBT', 2040),\n",
       " ('push', 2039),\n",
       " ('article', 2035),\n",
       " ('bill,', 2030),\n",
       " ('http://t.c…', 2029),\n",
       " ('oil', 2026),\n",
       " ('#smallbiz', 2026),\n",
       " ('finally', 2024),\n",
       " ('Look', 2021),\n",
       " ('today:', 2020),\n",
       " ('broken', 2018),\n",
       " ('cosponsor', 2017),\n",
       " ('Hearing', 2016),\n",
       " ('facility', 2014),\n",
       " ('\"I', 2010),\n",
       " ('years.', 2007),\n",
       " ('Need', 2006),\n",
       " (\"women's\", 2006),\n",
       " ('Chamber', 2004),\n",
       " ('p.m.', 2001),\n",
       " ('civil', 1996),\n",
       " ('question', 1995),\n",
       " ('updates', 1994),\n",
       " ('housing', 1993),\n",
       " ('Chair', 1988),\n",
       " ('East', 1987),\n",
       " ('Education', 1987),\n",
       " ('earlier', 1986),\n",
       " ('Is', 1985),\n",
       " ('heroes', 1983),\n",
       " ('cutting', 1982),\n",
       " ('family.', 1980),\n",
       " ('11', 1977),\n",
       " ('answer', 1977),\n",
       " ('God', 1975),\n",
       " ('sexual', 1972),\n",
       " ('entire', 1970),\n",
       " ('office.', 1968),\n",
       " ('administration', 1968),\n",
       " ('border', 1968),\n",
       " ('now:', 1966),\n",
       " ('#KeystoneXL', 1966),\n",
       " ('Co.', 1964),\n",
       " (\"tonight's\", 1963),\n",
       " ('priorities', 1963),\n",
       " ('Farm', 1962),\n",
       " ('20', 1956),\n",
       " ('led', 1956),\n",
       " ('manufacturing', 1952),\n",
       " ('supports', 1951),\n",
       " ('relief', 1951),\n",
       " ('agenda', 1950),\n",
       " ('#Trumpcare', 1950),\n",
       " ('officers', 1948),\n",
       " ('bringing', 1947),\n",
       " ('Rights', 1941),\n",
       " ('#LGBT', 1941),\n",
       " ('all.', 1940),\n",
       " ('risk', 1935),\n",
       " ('Sign', 1928),\n",
       " ('restore', 1926),\n",
       " ('innovation', 1926),\n",
       " ('Sen', 1926),\n",
       " ('heart', 1925),\n",
       " ('remarks', 1919),\n",
       " ('citizens', 1918),\n",
       " ('us.', 1918),\n",
       " ('ACA', 1918),\n",
       " ('represent', 1914),\n",
       " ('assistance', 1911),\n",
       " ('passes', 1910),\n",
       " ('events', 1910),\n",
       " ('While', 1909),\n",
       " ('month', 1907),\n",
       " ('common', 1907),\n",
       " ('released', 1906),\n",
       " ('Lake', 1905),\n",
       " ('independent', 1901),\n",
       " ('Army', 1896),\n",
       " ('treatment', 1896),\n",
       " ('incredible', 1895),\n",
       " ('#STEM', 1894),\n",
       " ('9', 1893),\n",
       " ('Obama’s', 1893),\n",
       " ('Amendment', 1893),\n",
       " ('FBI', 1890),\n",
       " ('River', 1889),\n",
       " ('His', 1889),\n",
       " ('America’s', 1885),\n",
       " ('victory', 1883),\n",
       " ('#ObamaCare', 1883),\n",
       " ('can’t', 1881),\n",
       " ('reports', 1880),\n",
       " ('legacy', 1879),\n",
       " ('invest', 1879),\n",
       " ('War', 1876),\n",
       " ('marks', 1875),\n",
       " ('mobile', 1871),\n",
       " ('Conference', 1869),\n",
       " ('play', 1867),\n",
       " ('delegation', 1866),\n",
       " ('demand', 1865),\n",
       " ('Year', 1863),\n",
       " ('stopped', 1860),\n",
       " ('Program', 1857),\n",
       " ('cancer', 1857),\n",
       " ('Tomorrow', 1854),\n",
       " ('Working', 1852),\n",
       " ('signing', 1852),\n",
       " ('transportation', 1851),\n",
       " ('Constitution', 1847),\n",
       " ('summer', 1844),\n",
       " ('ppl', 1843),\n",
       " ('posted', 1839),\n",
       " ('Mike', 1837),\n",
       " ('promote', 1836),\n",
       " ('#IA03', 1836),\n",
       " ('Water', 1833),\n",
       " ('Call', 1833),\n",
       " ('nomination', 1833),\n",
       " ('Force', 1832),\n",
       " ('taxpayers', 1832),\n",
       " ('forum', 1832),\n",
       " ('project', 1831),\n",
       " ('30', 1828),\n",
       " ('(1/2)', 1828),\n",
       " ('Friday', 1827),\n",
       " ('congressional', 1823),\n",
       " ('benefit', 1821),\n",
       " ('Yesterday', 1821),\n",
       " ('time.', 1807),\n",
       " ('behalf', 1803),\n",
       " ('Department', 1802),\n",
       " ('providing', 1801),\n",
       " ('fought', 1801),\n",
       " ('Stay', 1797),\n",
       " ('reform.', 1793),\n",
       " ('rates', 1793),\n",
       " ('Leadership', 1792),\n",
       " ('afford', 1792),\n",
       " ('Special', 1791),\n",
       " ('assault', 1790),\n",
       " ('(2/2)', 1787),\n",
       " ('living', 1786),\n",
       " ('communities.', 1783),\n",
       " ('Subcommittee', 1780),\n",
       " ('veteran', 1779),\n",
       " ('success', 1778),\n",
       " ('@SpeakerRyan', 1778),\n",
       " ('enjoy', 1777),\n",
       " ('me.', 1776),\n",
       " ('favorite', 1774),\n",
       " ('BREAKING:', 1773),\n",
       " ('secure', 1772),\n",
       " ('#Zika', 1771),\n",
       " ('colleague', 1766),\n",
       " ('recovery', 1766),\n",
       " ('receiving', 1764),\n",
       " ('roundtable', 1763),\n",
       " ('violence.', 1763),\n",
       " ('workforce', 1762),\n",
       " ('wrong', 1758),\n",
       " ('#Iran', 1755),\n",
       " ('Christmas', 1748),\n",
       " ('Admin', 1748),\n",
       " ('lunch', 1747),\n",
       " ('disaster', 1747),\n",
       " ('travel', 1747),\n",
       " ('Honor', 1747),\n",
       " ('healthy', 1744),\n",
       " ('Chief', 1743),\n",
       " ('terrorist', 1742),\n",
       " ('ago,', 1742),\n",
       " ('elected', 1741),\n",
       " ...]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec(sentences):\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "    num_features = 200\n",
    "    epoch_count = 10\n",
    "    sentence_count = len(sentences)\n",
    "    w2v_file = os.path.join(save_dir, \"word_vectors.w2v\")\n",
    "    word2vec = None\n",
    "    if os.path.exists(w2v_file):\n",
    "        print(\"w2v model loaded from \" + w2v_file)\n",
    "        word2vec = w2v.Word2Vec.load(w2v_file)\n",
    "    else:\n",
    "        word2vec = w2v.Word2Vec(sg=1,\n",
    "                                seed=1,\n",
    "                                workers=num_workers,\n",
    "                                size=num_features,\n",
    "                                min_count=min_frequency_val,\n",
    "                                window=5,\n",
    "                                sample=0)\n",
    "\n",
    "        print(\"Building vocab...\")\n",
    "        word2vec.build_vocab(sentences)\n",
    "        print(\"Word2Vec vocabulary length:\", len(word2vec.wv.vocab))\n",
    "        print(\"Training...\")\n",
    "        word2vec.train(sentences, total_examples=sentence_count, epochs=epoch_count)\n",
    "        print(\"Saving model...\")\n",
    "        word2vec.save(w2v_file)\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(word2vec):\n",
    "    all_word_vectors_matrix = word2vec.wv.syn0\n",
    "    num_words = len(all_word_vectors_matrix)\n",
    "    vocab = word2vec.wv.vocab.keys()\n",
    "    vocab_len = len(vocab)\n",
    "    dim = len(word2vec.wv[vocab]) #word2vec.wv[vocab[0]].shape[0]\n",
    "    dim = 200\n",
    "    embedding = np.empty((num_words, dim), dtype=np.float32)\n",
    "    metadata = \"\"\n",
    "    for i, word in enumerate(vocab):\n",
    "        embedding[i] = word2vec.wv[word]\n",
    "        metadata += word + \"\\n\"\n",
    "    metadata_file = os.path.join(save_dir, \"metadata.tsv\")\n",
    "    with io.open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(metadata)\n",
    "\n",
    "    reset_default_graph() #tf.reset_default_graph()\n",
    "    sess = tf.compat.v1.InteractiveSession()\n",
    "    X = tf.Variable([0.0], name='embedding')\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    place = tf.compat.v1.placeholder(tf.float32, shape=embedding.shape)\n",
    "    set_x = tf.compat.v1.assign(X, place, validate_shape=False)\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    sess.run(set_x, feed_dict={place: embedding})\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(save_dir, sess.graph)\n",
    "#     config = projector.ProjectorConfig()\n",
    "#     embedding_conf = config.embeddings.add()\n",
    "#     embedding_conf.tensor_name = 'embedding:0'\n",
    "#     embedding_conf.metadata_path = 'metadata.tsv'\n",
    "#     projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    save_file = os.path.join(save_dir, \"model.ckpt\")\n",
    "    print(\"Saving session...\")\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "Word2Vec vocabulary length: 122001\n",
      "Training...\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Instantiating word2vec model\")\n",
    "min_frequency_val = 6\n",
    "word2vec = get_word2vec(tweets_df.textgo_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec vocab contains 122001 items.\n",
      "word2vec items have 122001 features.\n"
     ]
    }
   ],
   "source": [
    "vocab = word2vec.wv.vocab.keys()\n",
    "vocab_len = len(vocab)\n",
    "print(\"word2vec vocab contains \" + str(vocab_len) + \" items.\")\n",
    "dim0 = len(word2vec.wv[vocab]) # word2vec.wv[vocab[0]].shape[0]\n",
    "print(\"word2vec items have \" + str(dim0) + \" features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madeline.campbell/.pyenv/versions/3.6.8/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (1,) and (122001, 200) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-abe7fd5162e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mset_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# sess.run(tf.compat.v1.global_variables_initializer())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# sess.run(set_x, feed_dict={place: embedding})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         validate_shape=validate_shape)\n\u001b[0;32m--> 228\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0mvalue_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m    860\u001b[0m           self.handle, value_tensor, name=name)\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \"\"\"\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (1,) and (122001, 200) are incompatible"
     ]
    }
   ],
   "source": [
    "all_word_vectors_matrix = word2vec.wv.syn0\n",
    "num_words = len(all_word_vectors_matrix)\n",
    "vocab = word2vec.wv.vocab.keys()\n",
    "vocab_len = len(vocab)\n",
    "dim = len(word2vec.wv[vocab]) #word2vec.wv[vocab[0]].shape[0]\n",
    "dim = 200\n",
    "embedding = np.empty((num_words, dim), dtype=np.float32)\n",
    "metadata = \"\"\n",
    "for i, word in enumerate(vocab):\n",
    "    embedding[i] = word2vec.wv[word]\n",
    "    metadata += word + \"\\n\"\n",
    "metadata_file = os.path.join(save_dir, \"metadata.tsv\")\n",
    "with io.open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(metadata)\n",
    "\n",
    "reset_default_graph() #tf.reset_default_graph()\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "X = tf.Variable([0.0], name='embedding')\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "place = tf.compat.v1.placeholder(tf.float32, shape=embedding.shape)\n",
    "set_x = tf.compat.v1.assign(X, place, validate_shape=False)\n",
    "# sess.run(tf.compat.v1.global_variables_initializer())\n",
    "# sess.run(set_x, feed_dict={place: embedding})\n",
    "\n",
    "# summary_writer = tf.summary.FileWriter(save_dir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(122001, 200) dtype=float32>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #[vocab[0]].shape[0]\n",
    "#len(word2vec.wv[vocab])\n",
    "#len(embedding[i])\n",
    "place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tensorboard embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madeline.campbell/.pyenv/versions/3.6.8/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (1,) and (122001, 200) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-85a544b8cfc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating tensorboard embeddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcreate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-126-8f5beed59cf6>\u001b[0m in \u001b[0;36mcreate_embeddings\u001b[0;34m(word2vec)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mset_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         validate_shape=validate_shape)\n\u001b[0;32m--> 228\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0mvalue_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m    860\u001b[0m           self.handle, value_tensor, name=name)\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \"\"\"\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (1,) and (122001, 200) are incompatible"
     ]
    }
   ],
   "source": [
    "print(\"Creating tensorboard embeddings\")\n",
    "create_embeddings(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(input_word, num_similar):\n",
    "    sim = word2vec.wv.most_similar(input_word, topn=num_similar)\n",
    "    output = []\n",
    "    found = []\n",
    "    for item in sim:\n",
    "        w, n = item\n",
    "        found.append(w)\n",
    "    output = [input_word, found]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_sne_scatterplot(word):\n",
    "    vocab = word2vec.wv.vocab.keys()\n",
    "    vocab_len = len(vocab)\n",
    "    dim0 = word2vec.wv[vocab[0]].shape[0]\n",
    "    arr = np.empty((0, dim0), dtype='f')\n",
    "    w_labels = [word]\n",
    "    nearby = word2vec.wv.similar_by_word(word, topn=num_similar)\n",
    "    arr = np.append(arr, np.array([word2vec[word]]), axis=0)\n",
    "    for n in nearby:\n",
    "        w_vec = word2vec[n[0]]\n",
    "        w_labels.append(n[0])\n",
    "        arr = np.append(arr, np.array([w_vec]), axis=0)\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=1)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "\n",
    "    plt.rc(\"font\", size=16)\n",
    "    plt.figure(figsize=(16, 12), dpi=80)\n",
    "    plt.scatter(x_coords[0], y_coords[0], s=800, marker=\"o\", color=\"blue\")\n",
    "    plt.scatter(x_coords[1:], y_coords[1:], s=200, marker=\"o\", color=\"red\")\n",
    "\n",
    "    for label, x, y in zip(w_labels, x_coords, y_coords):\n",
    "        plt.annotate(label.upper(), xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()-50, x_coords.max()+50)\n",
    "    plt.ylim(y_coords.min()-50, y_coords.max()+50)\n",
    "    filename = os.path.join(plot_dir, word + \"_tsne.png\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_t_sne():\n",
    "    vocab = word2vec.wv.vocab.keys()\n",
    "    vocab_len = len(vocab)\n",
    "    arr = np.empty((0, dim0), dtype='f')\n",
    "    labels = []\n",
    "    vectors_file = os.path.join(save_dir, \"vocab_vectors.npy\")\n",
    "    labels_file = os.path.join(save_dir, \"labels.json\")\n",
    "    if os.path.exists(vectors_file) and os.path.exists(labels_file):\n",
    "        print(\"Loading pre-saved vectors from disk\")\n",
    "        arr = load_bin(vectors_file)\n",
    "        labels = load_json(labels_file)\n",
    "    else:\n",
    "        print(\"Creating an array of vectors for each word in the vocab\")\n",
    "        for count, word in enumerate(vocab):\n",
    "            if count % 50 == 0:\n",
    "                print_progress(count, vocab_len)\n",
    "            w_vec = word2vec[word]\n",
    "            labels.append(word)\n",
    "            arr = np.append(arr, np.array([w_vec]), axis=0)\n",
    "        save_bin(arr, vectors_file)\n",
    "        save_json(labels, labels_file)\n",
    "\n",
    "    x_coords = None\n",
    "    y_coords = None\n",
    "    x_c_filename = os.path.join(save_dir, \"x_coords.npy\")\n",
    "    y_c_filename = os.path.join(save_dir, \"y_coords.npy\")\n",
    "    if os.path.exists(x_c_filename) and os.path.exists(y_c_filename):\n",
    "        print(\"Reading pre-calculated coords from disk\")\n",
    "        x_coords = load_bin(x_c_filename)\n",
    "        y_coords = load_bin(y_c_filename)\n",
    "    else:\n",
    "        print(\"Computing T-SNE for array of length: \" + str(len(arr)))\n",
    "        tsne = TSNE(n_components=2, random_state=1, verbose=1)\n",
    "        np.set_printoptions(suppress=True)\n",
    "        Y = tsne.fit_transform(arr)\n",
    "        x_coords = Y[:, 0]\n",
    "        y_coords = Y[:, 1]\n",
    "        print(\"Saving coords.\")\n",
    "        save_bin(x_coords, x_c_filename)\n",
    "        save_bin(y_coords, y_c_filename)\n",
    "    return x_coords, y_coords, labels, arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating T-SNE for word2vec model\")\n",
    "x_coords, y_coords, labels, arr = calculate_t_sne()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
